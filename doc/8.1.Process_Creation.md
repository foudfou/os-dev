# 8.1 Process Creation

**Note**: this iteration is captured in the git tag `process-creation`.

Our goal in this part is simply to create a process: create its metadata and
load its progam into memory. That is: we're not going to actually run it
yet. This we will do in a second step when implementing context switching.

## Overview

First, looking at
[hux](https://github.com/josehu07/hux-kernel.wiki/blob/b0a30aca0cd301e86ed8d52c498d9294fa1c892c/13.-Abstraction-of-Process.md)
and
[xv6-annotated](https://github.com/palladian1/xv6-annotated/blob/8e0b6e56cca723bd46f4c00612fb4eb420ac65a2/processes.md),
we can get an idea of what the kernel needs to keep track of, the main
components being: registers context, process page directory, stack pointer. The
kernel will keep that information in a **process table**: one *process control
block* (PCB) per process.

In short, as we learn in the
[littleosbook](https://littleosbook.github.io/#user-mode), setting up a process
looks like this:

- Allocate page frames for code/data and stack.
- Copy the program code to code page frames.
- Allocate and initialize the page directory and page tables for the new page frames.
  - PTEs will need `PTE_U`.
  - Kernel needs to be mapped in as well since we'll want to continue
    executing kernel code.

How about the IDT? Well, all interruption being handled by the OS, there's only
one: the one we already defined.

For the init process â€” the first user process started by the kernel, in essence
(leaving aside fork and trap logic), xv6 and hux do the following:

1. Find an unused slot in the process table. Allocate 1 page for the process
   kernel stack and reserve space on it for the process context
   (`allocproc()`). Update the process state `p->state = INITIAL` (aka `EMBRYO`
   in xv6).
1. Allocate 1 page for the process page directory and populate it by mapping
   the kernel space starting from KERNBASE (0x80000000) (`setupkvm()`). That's
   exactly what we did when entering paging mode.
1. Allocate 1 page for code/data, map it starting from 0x0 (`inituvm()`) and
   copy the init code over to it. Update the process state `p->state = READY`
   (aka `RUNNABLE` in xv6).

Just re-iterating here that, for simplicity, we map the whole pyhsical memory
into the kernel address space. Xv6 does roughly the same ("*It maps virtual
addresses KERNBASE:KERNBASE+PHYSTOP to 0:PHYSTOP.*") but is slightly more
precise in that it maps only specific the memory regions the kernel needs to
access: i/o space, kernel code+data, usable physical memory, more devices.

So overall, for each process the kernel will have to store:

- its address space: own page directory,
- context registers: the values of CPU registers when switching context,
- kernel stack: the whole execution environment in case of context switch
  (system call) or interrupt.

The process table is locked only for the single assignement of changing the
process state. As xv6 points out, the assignment is not atomic. There will be
more to say to the process table locking, especially in the context of multiple
CPUs, when we dive into scheduling in a later chapter FIXME.

We also need to clarify the process stacks. Let's also defer that discussion to
a later chapter. It's enough for now to quote from the xv6 book and explain the
implementation details later:

> Each process has two stacks: a user stack and a kernel stack (p->kstack).
> When the process is executing user instructions, only its user stack is in
> use, and its kernel stack is empty. When the process enters the kernel (for a
> system call or interrupt), the kernel code executes on the processâ€™s kernel
> stack; while a process is in the kernel, its user stack still contains saved
> data, but isnâ€™t actively used. A processâ€™s thread alternates between actively
> using its user stack and its kernel stack. The kernel stack is separate (and
> protected from user code) so that the kernel can execute even if a process
> has wrecked its user stack. [^xv6-book]


## Init process

Starting the first process, aka "init", is a bit of a catch-22 at this point:
we want to load the binary into memory, but don't have any filesystem and IDE
driver to read it from the disk. So what we can do in the meantime is embed the
bootstrap program (`initcode.asm`) into the kernel image. The kernel can later
reference it with the linker symbols `_binary_<objfile>_start`,
`_binary_<objfile>_end`, `_binary_<objfile>_size`. This naming convention comes
from `ld` and is actually [not well
documented](https://stackoverflow.com/a/29035523).

CAUTION the symbol name depends on the object file path:

> Note that the path to our `init` binary is named as `./src/process/init`, so
> the symbol names will be `_binary___src_process_init_start`, etc. [^hux-processes]

That's what hux does with a very minimal init program. Xv6 does a bit of the
same thing but the minimal program (`initcode.S`) makes an `exec()` syscall
which in turn will read `"/init"` from the disk.

For now, following hux, our intent is for the init program to only print a
character on the screen.

## Implementation

We shamelessly copy xv6, where code seemed split in logical blocks. The only
difference is that we map the whole physical memory.

We'll dive into a lot of small details afterwards, but specifically in the
process structure, why doesn't `struct context` contain all CPU registers?
[ShortÂ answer](https://github.com/palladian1/xv6-annotated/blob/8e0b6e56cca723bd46f4c00612fb4eb420ac65a2/processes.md?plain=1#L25):
because others either don't change between processes (flags register, control
registers, and GDT/IDT registers) or should already be saved by the caller (we
must be in some call frame) as per **cdecl** convention (EAX, ECX, and EDX, aka
"caller-saved registers", aka **volatile** registers).

### Virtual memory review

Maybe let's come back to virtual memory for a moment and make sure we really
understand how it works.

When setting up virtual memory, we prepare the kernel's page directory and map
addresses.

Right, so first the kernel page directory (`kpgdir`) is a pointer to 32-bits
entries, aliased `pde_t`. We allocate a frame (4Â KB) for it (`kalloc()`), which
is exactly what is needed for 1024 PDEs (nice eh?), initialize it with zeros
and make `kpgdir` point to it.

Then we map all the virtual memory range we're interested in. That's what we do
with `mappages()`: we map virtual addresses

    [KERNBASE, KERNBASE + (phys_end - 0)]

to physical addresses

    [0, phys_end]

We do that by iterating over blocks of 4Â KB (`PGSIZE`): for each block we
create the corresponding page table entry that's going to hold the mapping for
that block. That's what `walkpgdir()` does and the code is not easy to grasp
because there's a lot of weird computation going on. Let's break it down:

1. Unless it exists, allocate a frame for the page table. `kalloc()` returns a
   ~~physical~~ virtual memory address. Remember the linked list of free frames
   is threaded through the frames themselves which we initialized as virtual
   addresses. We did so because paging is actually already enabled when
   execution reaches the kernel's `main()`, which is needed for our higher half
   kernel.
1. Initialize the allocated frame with zeros.
1. Set the *corresponding* PDE (in the page directory) to the address of the
   new frame and appropriate flags, like `present` and `writable`. How do we
   find the corresponding PDE? Well we derive its *index* in the pagedir with
   `PDX()` which takes the first 10 (most significant) bits of the virtual
   address. We work backwards, as the TLB will work forwards ðŸ™‚. Reminder from
   xv6:

        A virtual address 'va' has a three-part structure as follows:

        +--------10------+-------10-------+---------12----------+
        | Page Directory |   Page Table   | Offset within Page  |
        |      Index     |      Index     |                     |
        +----------------+----------------+---------------------+
         \--- PDX(va) --/ \--- PTX(va) --/

   Remember the translation by the TLB works like this:
   - Lookup address of page dir.
   - Derive index in the page dir by taking first 10 bits from the virtual
     address; Lookup PDE thus page table *physical address*.
   - Derive index in the page table by taking next 10 bits from the virtual
     address; Lookup PTE thus final *physical address*.
1. Set the *corresponding* PTE to the final physical address and appropriate
   flags. To find the corresponding PTE, we derive its index in the page table
   with `PTX(va)` which takes the next 10 bits from the virtual address. In
   xv6, `walkpgdir()` actually returns the PTE address (`&pgtab[PTX(va)]`) to
   `mappages()` which initializes the PTE propely.

To nail it let's look at a couple examples, like the 2 first pages and the 2
last ones maybe.

Say the kernel heap's start points at 0x803ff000, growing downwards. Remember
the kernel heap's memory region starts right after the kernel's code on a 4Â KB
boundary at 0x8010a000. But since the free list is constructed by *freeing*
frames starting from the beginning of the address range, the first available
frame is located at the end.

`kpgdir` will point to the first allocated frame at 0x803ff000.

The first addresses we want to map are the first 4Â KB starting from
`KERNBASE`. Now following the process described previously, for mapping
virtual 0x80000000 â†’ physical 0x0, we:

- Check if PDE exists. PDE index for address 0x80000000 is 512 (0x200), so
  PDE's address is (0x803ff000+512*4-bytes) pde=0x803ff800. PDE's `present`
  flag is not set.
- Allocate a frame for the page table: pgtab=0x803fe000.
- Set the corresponding PDE to its physical address, which is 0x3fe000, plus
  some flags *pde=0x3fe007.
- Set the corresponding PTE to 0x80000000's physical address (0x0). PTE index
  for address 0x80000000 inside allocated page table is 0x0, so pte=0x803fe000,
  and we set it to 0x0 plus some flags *pte=0x3.

Fanstastic! We just made it so that when the TLB looks up say virtual
0x80000004, it:

- derives the corresponding PDE's address, pde=0x803ff800
  (0x803ff000+PDE_index*4), which indicates the physical address of the page
  table: 0x3fe000;
- derive the corresponding PTE's address, which is also 0x3fe000 since the PTE
  index is 0, which indicates the final physical address page 0x0;
- applies the address offset (0x4) to obtain the final physical address 0x4.

Let's now take the next block 0x80001000 â†’ 0x1000:

- PDE index for 0x80001000 is also 512. PDE is present (pde=0x803ff800) and we
  have allocated the corresponding page table already (pgtab=0x803fe000).
- All that's left to do is thus to set the corresponding PTE to 0x80001000's
  physical address (0x1000). PTE index for 0x80001000 inside page table is 0x1,
  so pte=0x803fe004, and we set it to 0x1000 plus some flags *pte=0x1003.

Say `phys_end` is 0x400000, so the last block is [0x803ff000,0x80400000).

- PDE index for 0x803ff000 is again 512. PDE present (pde=0x803ff800) no
  allocated needed.
- Set the corresponding PTE. PTE index for 0x803ff000 is 0x3ff (1023),
  pte=0x803feffc (0x803fe000+0x3ff*4), which we set to *pte=0x3ff003.


Here some supporting logs:

```
__mappages pgdir=0x803ff000

__walkpgdir1 pde=0x803ff800 *pde=0x0 PDX(va)=200
__walkpgdir2_2 pde=0x803ff800 *pde=0x3fe007 pgtab=0x803fe000
__walkpgdir3 pgtab_i=0x0 &pgtab[PTX(va)]=0x803fe000
__mappages a=0x80000000 pte=0x803fe000 *pte=0x3 pgdir_i=512 pde=0x803ff800

__walkpgdir1 pde=0x803ff800 *pde=0x3fe007 PDX(va)=200
__walkpgdir2_1 pde=0x803ff800 *pde=0x3fe007 pgtab=0x803fe000
__walkpgdir3 pgtab_i=0x1 &pgtab[PTX(va)]=0x803fe004
__mappages a=0x80001000 pte=0x803fe004 *pte=0x1003 pgdir_i=512 pde=0x803ff800

__walkpgdir1 pde=0x803ff800 *pde=0x3fe007 PDX(va)=200
__walkpgdir2_1 pde=0x803ff800 *pde=0x3fe007 pgtab=0x803fe000
__walkpgdir3 pgtab_i=0x1022 &pgtab[PTX(va)]=0x803feff8
__mappages a=0x803fe000 pte=0x803feff8 *pte=0x3fe003 pgdir_i=512 pde=0x803ff800

__walkpgdir1 pde=0x803ff800 *pde=0x3fe007 PDX(va)=200
__walkpgdir2_1 pde=0x803ff800 *pde=0x3fe007 pgtab=0x803fe000
__walkpgdir3 pgtab_i=0x1023 &pgtab[PTX(va)]=0x803feffc
__mappages a=0x803ff000 pte=0x803feffc *pte=0x3ff003 pgdir_i=512 pde=0x803ff800
```

Interestingly, for this address range (KERNBASE:KERNBASE+0x400000), all
mappings use the same PDE 0x803ff800.


### `freevm()`

One reason we're coming back to virtual memory is that, when setting up the
page directory (`setupkvm()`), may it be for the kernel (`paging_init()`) or
for the first user process (`initproc_init()`), xv6 takes the precaution to
free up allocated memory calling `freevm()` in case `mappages()` fails.

`freevm()`'s implementation looks peculiar at first. It proceeds in 2 steps:
deallocate user pages then free PTEs. Why don't we deallocate user pages while
iterating over pages? I.e. why do we need `deallocuvm()`?

Looking at the case of first user process, the question we can ask ourselves
here is: what exactly do we want to free here? Well the allocated frames. But
what frames were allocated exactly? Let's count allocated frames in
`initproc_init()` and where they're referenced:

| Call                        | Purpose              | Nb     | Reference                          |
|-----------------------------|----------------------|--------|------------------------------------|
| `process_alloc()`           | process kernel stack | 1      | `p->kstack`                        |
| `setupkvm()`                | page directory       | 1      | `p->pgdir`                         |
| `setupkvm()`                | page tables          | *some* | process pgdir                      |
| `inituvm()`                 | user process code    | 1      | *physical address for virtual 0x0* |
| `mappages()` in `inituvm()` | page table           | 1      | process pgdir                      |

As we see by enurating allocations, the alternate approach of recursively
freeing page directory entries (iterate over PDEs, iterate over PTEs, free
pages at all levels) doesn't work. For one, not all physical addresses point to
allocated frames (ex: kernel code). So how do we tell which physical addresses
are user pages? Xv6 approach is thus to start with user pages (`deallocuvm()`),
iterating over the process virtual address range [0, KERNBASE) and uses
`walkpgdir()` to find the addresses of related (allocated) pages.

Note also `freevm()` doesn't deallocate the process kernel stack, at least at
this stage.

Nit remark: `setupkvm()`, which is meant for the *kernel* part of the page
directory, uses `freevm()` in case of `mappages()` failure. But `freevm()` also
frees up all *user* code (`deallocuvm()`). Maybe we can come up with a better
split of responsabilities later?

Note linux-0.01 uses the recursive approach in `free_page_tables()` but only on
the current task's code and data segments, which actually resembles xv6's use
of `deallocuvm()`. Except `deallocuvm()` covers the whole process memory space.


## References

- [^xv6-book]: [xv6-book](https://pdos.csail.mit.edu/6.828/2018/xv6/book-rev11.pdf)
- [^hux-process]: [hux-process](https://github.com/josehu07/hux-kernel.wiki/blob/b0a30aca0cd301e86ed8d52c498d9294fa1c892c/13.-Abstraction-of-Process.md)
ion.md)
