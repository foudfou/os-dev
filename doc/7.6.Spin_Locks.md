# 7.6 Interlude — Spin Locks

As we're done for now with memory management, and before jumping into
processes, let's make a quick detour into concurrency and locks.

This chapter will assume prior knowledge about concurrency, atomic operations,
the *critical section*.

So far we have actually encountered spin locks in `cprintf()` (in
screen/console code) and `kalloc.c`.

> xv6 uses locks all over the place, so we're gonna have to get comfortable
> with them right away. Luckily, xv6 primarily uses spin-locks, which are super
> simple and work on bare metal; a lot of the more complex/more awesome locks
> that OSTEP talks about require an OS beneath them. [^xv6-annotated]

So what are spin-locks? They are simple™ locks, that work with a
hardware-provided atomic *test-and-set* operation (`xchg` on x86), and where
`acquire()` simply goes into a waiting loop ("spins"):

```C
  while(xchg(&lk->locked, 1) != 0)
    ;
```

Why spin locks? We need them to ensure the consistency of critical kernel data
structures, in the context of concurrency, multiple threads or processes,
possibly running on multiple CPUs (**Symmetric Multiprocessor** — SMP).

Now our OS will target single CPU/core architectures for now. Are spin locks
still relevant in this context?
[Generally speaking](https://stackoverflow.com/questions/1025859/is-spin-lock-useful-in-a-single-processor-uni-core-architecture),
no. But OSTEP goes into more details:

> To work correctly on a single processor, it requires a preemptive scheduler
> (i.e., one that will interrupt a thread via a timer, in order to run a
> different thread, from time to time). Without preemption, spin locks don’t
> make much sense on a single CPU, as a thread spinning on a CPU will never
> relinquish it. [^OSTEP-locks]

And fortunately (as we really want to learn about these locks), xv6, from which
we want to draw inspiration, is preemptive:

> xv6 implements a round-robin preemptive scheduler with a fixed quantum. In
> other words, the scheduler picks a process and runs it for a specific fixed
> amount of clock ticks, it then forces that process to yield if it is not
> done, and then schedules the next process to run from the list of
> processes. [CSSE 332](https://rhit-csse.github.io/csse332/docs/2122b_project2/)

Interestingly linux-0.10 doesn't seem to use them, and hux takes the easier
approach of simply disabling interrupts for the critical section. This is also
the first approach mentioned in OSTEP [^OSTEP-locks].

The [chapter on spin-locks in
xv6-annotated](https://github.com/palladian1/xv6-annotated/blob/8e0b6e56cca723bd46f4c00612fb4eb420ac65a2/spin_locks.md)
is an amazing read. We will just summarize its main points here.

## General considerations

There are mainly 3 important considerations for spin locks in particular:

- Test (if lock is available) *then* set (the lock), required for `acquire()`
  must happen in a single step, i.e. **atomic** operation. This is much easier
  and efficient when supported by the hardware . Hence `XCHG` in x86. Note
  `CMPXCHG` does exactly what we need but was introduced in i486, whereas xv6
  targets i386+.
- We need to disable interrupts for the current CPU (the one running the
  current code), during the critical section, i.e. disable interrupts in
  `acquire()` and re-enable them in `release()`.
- **Deadlocks** happen when 2 threads have to acquire multiple locks: one
  process acquires lock A while the other one acquires lock B, but both need
  locks A and B. The solution is to make sure all locks are acquired *in the same
  order*.

    > To avoid such deadlocks, all code paths must acquire locks in the same
    > order.  The need for a global lock acquisition order means that locks are
    > effectively part of each function’s specification: callers must invoke
    > functions in a way that causes locks to be acquired in the agreed-on
    > order. [^xv6-book]

Because of the overhead introduced by locks in general, there are many other
consideration as to the granularity of the objects they should protect or when
to use them. An important one that especially applies to spin locks is when to
acquire and release:

> This means we should be careful when using locks to acquire them only at the
> last possible moment when they're absolutely needed, and release them as soon
> as they're no longer required, in order to limit the amount of wasted CPU
> cycles. [^xv6-annotated]

### XCHG

Let's zoom-in into `xchg` for a moment: how does this swap operation enable
atomic locking?

> In one atomic operation, xchg swaps a word in memory with the contents of a
> register. The function acquire (1574) repeats this xchg instruction in a
> loop; each iteration atomically reads lk->locked and sets it to 1 (1581). If
> the lock is already held, lk->locked will already be 1, so the xchg returns 1
> and the loop continues. If the xchg returns 0, however, acquire has
> successfully acquired the lock—locked was 0 and is now 1—so the loop can
> stop. [^xv6-book]

To be super clear: we're not test-and-set'ing in an atomic operation, really
we're replacing atomically, which effectively enables atomic locking. Why? The
swap operation returns either 0 or 1. 0 then we acquired the lock successfully,
1 then someone acquired it before already, and we're thus simply overriding the
lock with its previous value, a kind of no-op.

Note set-then-test with normal load/store instructions doesn't work either
because there would be no way to test the original value and thus to aquire the
lock.

Note atomic hardware-based locking instructions have been introduced pretty
early in computer history:

> Because disabling interrupts does not work on multiple processors, and
> because simple approaches using loads and stores (as shown above) don’t work,
> system designers started to invent hardware support for lock- ing. The
> earliest multiprocessor systems, such as the Burroughs B5000 in the early
> 1960’s [M82], had such support; today all systems provide this type of
> support, even for single CPU systems. [^OSTEP-locks]

## Implementation

We will shamelessly use xv6's implementation. It carefully handles multiple
caveats:

- When disabling interrupts, we need to store the depth of calls and if we were
  inside an interrupt (`FL_IF`) in the first call, so we can restore the
  original state when we pop the call stack. Hence `pushcli()` and
  `popcli()`. The state is stored inside the cpu state structure.
- Make sure the compiler or processor does not re-order instructions in
  `acquire()` and `release()`:

    > Many compilers and processors, however, execute code out of order to
    > achieve higher performance. […] To tell the hardware and compiler not to
    > perform such re-orderings, xv6 uses `__sync_synchronize()`, in both
    > acquire and release. `_sync_synchronize()` is a memory barrier: it tells
    > the compiler and CPU to not reorder loads or stores across the
    > barrier. Xv6 worries about ordering only in acquire and release, because
    > concurrent access to data structures other than the lock structure is
    > performed between acquire and release. [^xv6-book]

Other implementation considerations:

- The `struct spinlock` structure used in `acquire()` and `release()` stores
  additional debugging information: the current cpu and the call stack (EIPs /
  PCs program counters). Hence `getcallerpcs()`, which is almost exactly our
  `stack_trace()`. This is why we refactor all this by replacing `panic()` with
  xv6's which makes use of `getcallerpcs()`.
- The spinlock code requires code related to cpu handling (`mycpu()`) and x86
  instruction wrappers (like `xchg()`) which we'll simply copy over and
  simplify.

## Conclusion

Finally we're able to use spin locks in existing code, namely `kalloc.c` and
`screen.c` \o/


## References

- [^xv6-annotated]: [xv6-annotated](https://github.com/palladian1/xv6-annotated/blob/main/spin_locks.md)
- [^OSTEP-locks]: [OSTEP-locks](https://pages.cs.wisc.edu/~remzi/OSTEP/threads-locks.pdf)
- [^xv6-book]: [xv6-book](https://pdos.csail.mit.edu/6.828/2018/xv6/book-rev11.pdf)
